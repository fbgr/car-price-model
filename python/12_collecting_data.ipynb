{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8304b61",
   "metadata": {},
   "source": [
    "# 1. Collecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4daa06d",
   "metadata": {},
   "source": [
    "## 1.1 Importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1c3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import datetime\n",
    "import concurrent\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import re\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a123a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "provincias_dict = {\n",
    "    \"north\": [\"coruna+a\", \"lugo\", \"ourense\", \"pontevedra\",\"asturias\",\"cantabria\",\"alava\", \"guipuzcoa\", \"vizcaya\",\"navarra\",\"rioja+la\"],\n",
    "    \"north_centre\":  [\"avila\", \"burgos\", \"leon\", \"palencia\", \"salamanca\", \"segovia\", \"soria\", \"valladolid\", \"zamora\",\"huesca\", \"teruel\", \"zaragoza\"],\n",
    "    \"south_centre\": [\"badajoz\", \"caceres\",\"albacete\", \"ciudad+real\", \"cuenca\", \"guadalajara\", \"toledo\"],\n",
    "    \"south\": [\"almeria\", \"cadiz\", \"cordoba\", \"granada\", \"huelva\", \"jaen\", \"malaga\",\"palmas+las\", \"tenerife\"],\n",
    "    \"east\": [\"murcia\", \"castellon\", \"valencia\",\"girona\", \"lleida\", \"tarragona\", \"illes+balears\"],\n",
    "    \"big_capitals\": ['madrid','barcelona','sevilla','alicante'],\n",
    "    \"prueba\": ['huesca','teruel']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f41f1b",
   "metadata": {},
   "source": [
    "## 1.2 Custom functions to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf44cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to(link, sleeptime, max_tries=3):\n",
    "    \"\"\"\n",
    "    Attempts to connect to a car listing. Some links are mispelled (it's the web fault),\n",
    "    so we could have an error. The function tries to connect a max number of #tries.\n",
    "    Args:\n",
    "    * link (string): The link of the car listing.\n",
    "    * sleeptime (float): The pproximated number seconds to sleep between connections.\n",
    "    * max_tries (int): The max. number of times it tries to connect.\n",
    "    Returns:\n",
    "    * response or string: If there was a connection, the return is a response. Otherwise it returns 'Failed'.\n",
    "    \"\"\"\n",
    "\n",
    "    tries = 0\n",
    "    while True:\n",
    "        \n",
    "        try: # connecting\n",
    "            response = requests.get(link)\n",
    "            break\n",
    "                    \n",
    "        except: # error connection\n",
    "            tries += 1\n",
    "            if tries > max_tries:\n",
    "                return 'Failed'            \n",
    "            print(' '*100,end='\\r')\n",
    "            print(f\"({datetime.datetime.now().hour}:{datetime.datetime.now().minute}|{tries}) Reconnecting to web...\", end='\\r')\n",
    "            sleep(abs(np.random.normal(sleeptime+tries,0.15)))\n",
    "            continue\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4ca590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(css_path,numerical,soup):\n",
    "    \"\"\"\n",
    "    Extracts a feature from the html of a car listing.\n",
    "    Args:\n",
    "    * css_path (string): The css path towards the feature.\n",
    "    * numerical (boolean): Equal to True if the feature is numerical, otherwise false.\n",
    "    * soup (bs4.BeautifulSoup): Contains the html information of the car listing.\n",
    "    Returns:\n",
    "    * float or string: If the feature is numerical, it returns a float. Otherwise it returns a string.\n",
    "    \"\"\"\n",
    "    if numerical == 'True':\n",
    "        feature = soup.select(css_path)[0]\n",
    "        feature = re.findall(r'\\d+[.]?[,]?\\d*',feature.get_text().strip())[0].replace('.','').replace(',','.')\n",
    "    elif numerical == 'False':\n",
    "        feature = soup.select(css_path)[0].get_text().strip()\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf64e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sales_data(location,car_class,n_pages,sleeptime):\n",
    "    \"\"\"\n",
    "    Gets the sales data for a car class in a specific location.\n",
    "    Args:\n",
    "    * location (string): The location (province).\n",
    "    * car_class (string): The car class ('standard','sport','commercial' or '4x4').\n",
    "    * n_pages (int or string): The number of pages that will be read. Use 'auto' to automatically read all.\n",
    "    Returns:\n",
    "    * pd.DataFrame: Table containing the sales data for the specified conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We create the empty database where we'll add car samples.\n",
    "    data = pd.DataFrame(columns=['year','cv','km','fuel','doors','gearbox','emissions',\\\n",
    "                                 'color','warranty','seller','id','brand','price','boot',\\\n",
    "                                 'length','height','width','seats','max_sp','cmixto',\\\n",
    "                                'curban','extraurban','0-100','autonomy','displac','cylinders',\\\n",
    "                                'transmission','max_par','gear','class','location','link'])\n",
    "\n",
    "    \n",
    "    # Reading CSS Path for features from the HTML table.\n",
    "    with open(\"../data/css_paths.txt\", \"r\") as f:\n",
    "        css_paths = json.load(f)\n",
    "        \n",
    "    paths = [list_css[0] for list_css in css_paths.values()] \n",
    "    # The following list states if the feature is numerical ('True'), otherwise 'False'\n",
    "    paths_bool = [list_css[1] for list_css in css_paths.values()] \n",
    "    \n",
    "    # Rewriting car_classes for web search compatibility.\n",
    "    if car_class == 'standard':\n",
    "        class_string = 'pequeno-mediano-grande-familiar-monovolumen'\n",
    "    elif car_class == '4x4':\n",
    "        class_string = 'coches-4x4-todoterreno'\n",
    "    elif car_class == 'commercial':\n",
    "        class_string = 'furgonetas-segunda-mano'\n",
    "    elif car_class == 'sport':\n",
    "        class_string = 'coches-deportivos-segunda-mano'\n",
    "    else:\n",
    "        print('ERROR. The provided class does not exist. Please choose \"standard\",\"4x4\",\"commercial\" or \"sport\"')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    ncar = 0 # Numbers of cars added to the database.\n",
    "    skipped = 0 # Number of skipped cars.\n",
    "    \n",
    "    # This is the baselink, from which we we'll create each page link:\n",
    "    baselink = \"https://www.coches.com/coches-segunda-mano/\"+class_string+\"-en-\"+location+\".htm?page=\"\n",
    "    \n",
    "    # If n_pages == 'auto', we will search 499 pages (the maximum available). Otherwise, we go for the specified number.\n",
    "    max_page = 499\n",
    "    if n_pages != 'auto':\n",
    "        max_page = n_pages\n",
    "        \n",
    "    # --- ITERATION OVER EACH PAGE ---\n",
    "    for i in range(max_page):\n",
    "        \n",
    "        # We create the link for this page\n",
    "        link = baselink+str(i)      \n",
    "        \n",
    "        # Connecting to the web page\n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(link)\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                print(' '*100,end='\\r')\n",
    "                print('Reconnecting to next page...', end='\\r')\n",
    "                sleep(abs(np.random.normal(sleeptime+4,0.15)))\n",
    "                continue\n",
    "        \n",
    "        # Extracting HTML and searching for cars posted in the page.\n",
    "        soup_page = BeautifulSoup(response.content, \"html.parser\")\n",
    "        sleep(abs(np.random.normal(sleeptime,0.15)))\n",
    "        car_links = soup_page.select(\"html body main.content-page div#vo-results.vo-results--rebranding div.pillList.vo-results__card-list.script__vo-results-card-list div.cc-car-card.vo-results__card.pill.script__pill\")\n",
    "        \n",
    "\n",
    "        # --- ITERATION OVER EACH CAR --- \n",
    "        for car_link in car_links:\n",
    "            \n",
    "            if car_link.a['href'] == '':\n",
    "                break # If this card has no car associated, move on\n",
    "            \n",
    "            # Extract ID from the car\n",
    "            car_id = re.findall('id=([\\d]+)', car_link.a['href'])[0]\n",
    "            \n",
    "            # Connect to the car listing (try 2 times, otherwise move on)   \n",
    "            response = connect_to(car_link.a['href'],sleeptime=sleeptime)\n",
    "            if response == 'Failed':\n",
    "                skipped += 1\n",
    "                print(' '*100,end='\\r')\n",
    "                print(f'Skipping one car (total = {skipped})',end='\\r')\n",
    "                break\n",
    "            \n",
    "            # Extract HTML from the car listing\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            sleep(abs(np.random.normal(sleeptime,0.15)))\n",
    "            \n",
    "            # Read basic features\n",
    "            features_html = soup.select(\".cc-car-overview\")\n",
    "            if len(features_html)==0:\n",
    "                break # If we can't find the basic features, skip the car\n",
    "                \n",
    "            features = features_html[0].select(\"p\")\n",
    "            car_features = [code.get_text() for i,code in enumerate(features) if (i+1)%2==0]\n",
    "\n",
    "            # Read the car brand from the title\n",
    "            car_brand = soup.select(\"h1.index-card__make-model\")[0].get_text()\n",
    "            car_brand = re.findall(r\"[\\w]+\",car_brand)[0]\n",
    "                \n",
    "            # Obtain the price as the max of the prices (we are not interested in prices associated with bank loans)\n",
    "            car_price = [a.get_text() for a in soup.select(\"div.index-card__price-number\")] \n",
    "            car_prices = [int(re.findall(r'\\d+[.]*\\d*',price)[0].replace(\".\",\"\")) for price in car_price if len(re.findall(r'\\d+[.]*\\d*',price))>0]\n",
    "            \n",
    "            # Read additional features              \n",
    "            some_features = [extract_feature(path,var_bool,soup=soup) for path, var_bool in zip(paths,paths_bool)]\n",
    "            \n",
    "            \n",
    "            # Concatenating them to form the new list with the car features\n",
    "            more_features = [car_id, car_brand, np.max(car_prices)]+some_features+[car_class, location, car_link.a['href']]\n",
    "            car_features = car_features + more_features\n",
    "            \n",
    "            # Storing the car features in the database\n",
    "            data.loc[ncar] = car_features\n",
    "            ncar += 1\n",
    "            \n",
    "        # --- FINISHED ITERATION OVER EACH CAR --- \n",
    "        \n",
    "        if n_pages == 'auto':\n",
    "            \n",
    "            # Check what's the next page link\n",
    "            try:\n",
    "                next_page = soup_page.select(\".pager-next\")[0].a['href']\n",
    "            except:\n",
    "                # sometimes we only have 1 page, and so there's no next button.In that case, we're also in the last page:\n",
    "                next_page = link\n",
    "            \n",
    "            # Check if we reached the limit\n",
    "            if next_page == link:\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            # Show progress if n_pages != 'auto'\n",
    "            print(' '*100,end='\\r')\n",
    "            print(f'{(round((i+1)/n_pages*100,1))}% complete ({data.shape[0]} items).',end='\\r')\n",
    "            \n",
    "    # --- FINISHED ITERATION OVER EACH PAGE ---\n",
    "    \n",
    "    print('                                                                                ',end='\\r')\n",
    "    print(f\"Obtained {data.shape[0]} items of class '{car_class}' in {location.capitalize()}.\",end='\\r')   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7715f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regional_data(location,n_pages,sleeptime):\n",
    "    car_classes = ['sport','commercial','4x4','standard']\n",
    "    \n",
    "    car_data = [get_sales_data(location=location,car_class=car_class, n_pages = n_pages, sleeptime = sleeptime) for car_class in car_classes]\n",
    "    \n",
    "    car_data = pd.concat(car_data,axis=0).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Regional data from {location.capitalize()} scanned succesfully for a total of {car_data.shape[0]} items.\")   \n",
    "    \n",
    "    return car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e89ce292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_community_data(comunidad, n_pages = 'auto', sleeptime = 1.5, save_csv = False):\n",
    "    provincias = provincias_dict[comunidad]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        reg_cars_futures = [executor.submit(lambda x: get_regional_data(x,sleeptime = 0, n_pages = n_pages), prov) for prov in provincias]\n",
    "        reg_cars = [future.result() for future in concurrent.futures.as_completed(reg_cars_futures)]\n",
    "        regional_cars = pd.concat(reg_cars,axis=0).reset_index(drop=True)\n",
    "    \n",
    "    if save_csv == True:\n",
    "        regional_cars.to_csv('../data/'+comunidad+'.csv',index=False)\n",
    "    print(f\"All data from {comunidad.capitalize()} saved succesfully with a total of {regional_cars.shape[0]} items.\")   \n",
    "     \n",
    "    return regional_cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80b95b",
   "metadata": {},
   "source": [
    "# East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ae37ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regional data from Illes+balears scanned succesfully for a total of 1219 items.                     \n",
      "Regional data from Lleida scanned succesfully for a total of 2436 items.        \n",
      "Regional data from Tarragona scanned succesfully for a total of 8697 items.                         \n",
      "Regional data from Girona scanned succesfully for a total of 9256 items.        \n",
      "Regional data from Murcia scanned succesfully for a total of 11894 items.       \n",
      "Regional data from Castellon scanned succesfully for a total of 12956 items.    \n",
      "Regional data from Valencia scanned succesfully for a total of 18851 items.                         \n",
      "All data from East saved succesfully with a total of 65309 items.\n",
      "CPU times: user 7h 39min 45s, sys: 4min 40s, total: 7h 44min 26s\n",
      "Wall time: 9h 42min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "east = get_community_data('east', sleeptime = 0, save_csv = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6caea",
   "metadata": {},
   "source": [
    "# South"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874caa94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "south = get_community_data('south', sleeptime = 0, save_csv = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f234ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datetime.datetime.now().hour}:{datetime.datetime.now().minute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b62ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
