{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd27bd55",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e7a61",
   "metadata": {},
   "source": [
    "## 3.1 Importing useful libraries & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2713fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.regularizers import L2\n",
    "# evaluating\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a3069",
   "metadata": {},
   "source": [
    "## 3.2 Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6436f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('./data/cleaned_cars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e84ea4",
   "metadata": {},
   "source": [
    "## 3.3 Train, CV, Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f297e8",
   "metadata": {},
   "source": [
    "We will divide our data into three datasets. The **training** dataset will teach the model how to operate, the **cross-validation (CV)** dataset will help us optimize the parameters of the models, and finally the **test** dataset will be used to evaluate our final mode and estimate its error and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars.drop('price',axis=1)\n",
    "y = cars['price']\n",
    "\n",
    "# Creating train dataset\n",
    "X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.4)\n",
    "X_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Creating test & cv dataset\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_, y_, test_size=0.5)\n",
    "X_cv.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)\n",
    "y_cv.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2c08b",
   "metadata": {},
   "source": [
    "I split the features into numerical and categorical, so I can later normalize and one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training dataset\n",
    "X_num = X_train.select_dtypes(np.number)\n",
    "X_cat = X_train.select_dtypes(object)\n",
    "\n",
    "# 2. CV dataset\n",
    "X_num_cv = X_cv.select_dtypes(np.number)\n",
    "X_cat_cv = X_cv.select_dtypes(object)\n",
    "\n",
    "# 3. Test dataset\n",
    "X_num_test = X_test.select_dtypes(np.number)\n",
    "X_cat_test = X_test.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e006222",
   "metadata": {},
   "source": [
    "## 3.4 Normalizing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_scale(numericals, transformer):\n",
    "    normalized = []\n",
    "    for X_numerical in numericals:\n",
    "        X_normalized = transformer.transform(X_numerical)\n",
    "        X_normalized = pd.DataFrame(X_normalized, columns=X_numerical.columns)\n",
    "        normalized.append(X_normalized)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining normalizer\n",
    "transformer = StandardScaler().fit(X_num)\n",
    "\n",
    "# Applying normalizer\n",
    "X_norm, X_norm_cv, X_norm_test = std_scale([X_num,X_num_cv,X_num_test], transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090878c",
   "metadata": {},
   "source": [
    "## 3.5 Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c43c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(categoricals, encoder):\n",
    "    onehots = []\n",
    "    for X_categorical in categoricals:\n",
    "        encoded = encoder.transform(X_categorical).toarray()\n",
    "        onehot_encode = pd.DataFrame(encoded,columns=encoder.get_feature_names_out(X_categorical.columns))\n",
    "        onehots.append(onehot_encode)\n",
    "    return onehots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating encoder\n",
    "encoder = OneHotEncoder(handle_unknown='error',drop='first').fit(X_cat)\n",
    "\n",
    "# Applying onehot-encode\n",
    "X_oh, X_oh_cv, X_oh_test = one_hot([X_cat,X_cat_cv,X_cat_test], encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedff0d9",
   "metadata": {},
   "source": [
    "## 3.6 Concatenating back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.concat([X_norm, X_oh], axis=1)\n",
    "X_cv_scaled = pd.concat([X_norm_cv, X_oh_cv], axis=1)\n",
    "X_test_scaled = pd.concat([X_norm_test, X_oh_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3875b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f02af",
   "metadata": {},
   "source": [
    "## 3.7 Training models & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f40de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = 5e3\n",
    "model1 = LinearRegression()\n",
    "model2 = KNeighborsRegressor()\n",
    "model3 = DecisionTreeRegressor()\n",
    "model4 = RandomForestRegressor()\n",
    "model5 = XGBRegressor()\n",
    "model6 = Sequential(\n",
    "[\n",
    "    tf.keras.layers.Dense(256, activation = 'relu', name='L1', kernel_regularizer=L2(lmbd)),\n",
    "    tf.keras.layers.Dense(256, activation = 'relu', name='L12', kernel_regularizer=L2(lmbd)),\n",
    "    tf.keras.layers.Dense(256, activation = 'relu', name='L13', kernel_regularizer=L2(lmbd)),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu', name='L2', kernel_regularizer=L2(lmbd)),\n",
    "    tf.keras.layers.Dense(1, activation = 'relu', name='L7'),\n",
    "\n",
    "]\n",
    ")\n",
    "model6.compile(loss = MeanSquaredError(),optimizer = tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "model_pipeline = [model1, model2, model3, model4, model5, model6]\n",
    "model_names = ['Linear Regression', 'KNN','Decision Tree Regressor', 'RandomForest', 'XGBoost', 'NeuralNetwork']\n",
    "\n",
    "scores = {}\n",
    "for model, model_name in zip(model_pipeline, model_names):\n",
    "    print('Working with model '+model_name)\n",
    "    \n",
    "    # Fitting the model\n",
    "    if model_name == 'NeuralNetwork':\n",
    "        model.fit(X_train_scaled, y_train, epochs = 150,verbose=0)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_train_scaled)\n",
    "    training_error = r2_score(y_pred,y_train)\n",
    "    \n",
    "    y_pred_cv = model.predict(X_cv_scaled)\n",
    "    cv_error = r2_score(y_pred_cv,y_cv)\n",
    "    scores[model_name] = [round(training_error,3), round(cv_error,3)]\n",
    "    \n",
    "print(scores)\n",
    "# We can use the result to choose the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_pred_cv,0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "error =int((abs(np.round(y_pred_cv,0).flatten()-y_cv)/np.round(y_pred_cv,0).flatten()*100).astype(int).values.mean())\n",
    "std = int((abs(np.round(y_pred_cv,0).flatten()-y_cv)/np.round(y_pred_cv,0).flatten()*100).astype(int).values.std())\n",
    "print('The error is ',error,'Â±',4*std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c45ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!> I could try to use log or sqrt of some qualities like cv and more that seem to over emphasize \n",
    "# the price of high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc51028",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.round(y_pred_cv,0).flatten()-y_cv).astype(int).values[abs(np.round(y_pred_cv,0).flatten()-y_cv).astype(int).values>6000].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cv[abs(np.round(y_pred_cv,0).flatten()-y_cv).astype(int).values>6000]).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cv[abs(np.round(y_pred_cv,0)-y_cv).astype(int).values<6000]).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce26ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(np.round(y_pred_cv,0)-y_cv).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe10073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566962d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'marchas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd906159",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(X_cv[X_cv[a]==0.])/X_cv.shape[0]*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d158aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(X_train[X_train[a]==0.])/X_train.shape[0]*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17330a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv[a]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1715a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
